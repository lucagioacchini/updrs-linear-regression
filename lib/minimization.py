# !/usr/bin/env python2
#  -*- coding: utf-8 -*-

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


class SolveMinProb:
	"""Linear regression algorithms implementation	
		
		Args:
			y: column Numpy vector used in linear regression formula
			A: Numpy matrix used in linear regression formula
	
		Attributes:
			Np: (int) number of y and A rows (number of patients)
			Nf: (int) number of y and A column (number of column)
			matr: Np-by-Nf Numpy matrix
			vect: Np-by-1 Numpy vector
			sol: Nf-by-1 Numpy vector. Optimum vector generated by
				the algorithms
	"""
	def __init__(self, y, A):
		self.Np = y.shape[0]
		self.Nf = A.shape[1]
		self.matr = A
		self.vect = y
		self.sol = np.zeros((self.Nf,1),dtype=float)
		
	def plot_w(self, title, path):
		"""Plot the generated w vector.
		
		Args:
			title: (string) algorithm plot title and file name
			path: (string) folder path for saving plots
		"""
		w = self.sol
		plt.figure()
		plt.plot(np.arange(self.Nf), w, 'o')
		plt.xlabel('n')
		plt.ylabel('w(n)')
		plt.title(title+" optimized vector")
		plt.grid()
		plt.savefig(path+title+"_opt_vect.png")
	
	def save_data(self, path, title):
		"""Save dataframe as a csv.file.
		
		Args:
			path: (string) folder path for saving files
			title: (string) algorithm file name
		"""
		dataFrame = pd.DataFrame({title:self.err[:,0]})
		dataFrame.to_csv(path+'error_'+title+'.dat')
		
	def print_result(self, title):
		"""Print the result of the generated w vector
		
		Args:
			title: (string) algorithm name
		"""
		print title + ":\nThe optimum weight vector is:\n" + str(self.sol)
	
	def plot_err(self, title, path):
		"""Plot error function
		
		Args:
			title: (string) algorithm plot title
			path: (string) folder path for saving plots
		"""
		error = self.err
		plt.figure()
		plt.plot(error[:])
		plt.xlabel("N_it")
		plt.ylabel("err")
		plt.title(title+" error")
		plt.grid()
		plt.show()
		# plt.savefig(path+title+"_error.png")	
			
				
class SolveLLS (SolveMinProb):
	"""LLS Pseudoinverse algorithm implementation
	"""
	def run(self):
		"""Algorithm implementation
		"""
		# set the input parameters
		A = self.matr
		y = self.vect

		# determine the optimized vector
		w = np.dot(np.dot(np.linalg.inv(np.dot(A.T, A)), A.T), y)
		self.sol = w
		
		self.err = np.linalg.norm(np.dot(A, w) - y)**2 # apply the LLS formula


class SolveGrad (SolveMinProb):
	"""Gradient Descent algorithm implementation
	"""
	def run(self, gamma, Nit):
		"""Algorithm implementation
		
		Args:
			gamma: (float) learning coefficient
			Nit: (int) number of iterations
		"""
		# initialize the parameters
		self.err = np.zeros((Nit, 1), dtype = float)
		# set the input parameters
		A = self.matr
		y = self.vect
		w = np.random.rand(self.Nf, 1)
		
		# algorithm implementation
		for i in range (Nit):
			grad = 2*np.dot(A.T, (np.dot(A, w)-y))
			w = w - gamma*grad
			self.err[i] = np.linalg.norm(np.dot(A, w) - y)**2	
		
		self.sol = w	# assign the optimized vector to plot it
	
	
	def optimization(self, gamma, Nit, y_val, A_val):
		"""Find the optimum iterations number by checking the validation
		error.
		
		Args:
			gamma: (float) learning coefficient
			Nit: (int) number of iterations
			y_val: column Numpy vector of validation dataset
			A_val: Numpy matrix of validation dataset
		
		Returns:
			opt_Nit: (int) pptimum iterations number
		"""
		# initialize the parameters
		self.val_err = np.zeros((Nit, 1), dtype = float)
		self.train_err = np.zeros((Nit, 1), dtype = float)
		
		# number of iterations optimization
		for i in range(Nit):
			self.run(gamma, i)
			self.val_err[i] = np.linalg.norm(np.dot(A_val, self.sol) - y_val)**2
			self.train_err[i] = np.linalg.norm(np.dot(self.matr, self.sol) - self.vect)**2
			
		opt_Nit = np.argmin(self.val_err)		
		
		return opt_Nit
		
class SolveSteepDesc (SolveMinProb):
	"""Steepest Descent algorithm implementation.
	"""
	def run(self, Nit):
		"""Algorithm implementation
		
		Args:
			Nit: (int) Number of iterations
		"""
		# initialise a 0s array for the error
		self.err = np.zeros((Nit, 1), dtype = float)
		# set the input parameters
		A = self.matr
		y = self.vect
		w = np.random.rand(self.Nf, 1)

		# algorithm implementation
		for i in range(Nit):
			grad = 2*np.dot(A.T, (np.dot(A, w)-y))
			hess = 4*np.dot(A.T, A)
			gamma = np.linalg.norm(grad)**2 / np.dot(np.dot(grad.T, hess), grad)
			w = w - gamma*grad
			self.err[i] = np.linalg.norm(np.dot(A, w) - y)**2
		
		# assign the optimized vector to plot it
		self.sol = w


class StocGrad(SolveMinProb):
	""" Stochastic Gradient Descent implementation.
	"""
	def run(self, gamma, Nit):
		"""Algorithm implementation
		
		Args:
			gamma: (float) learning coefficient
			Nit: (int) number of iterations
		"""
		# initialise a 0s array for the error
		self.err = np.zeros((Nit, 1), dtype = float)
		# set the input parameters
		A = self.matr
		y = self.vect
		w = np.random.rand(self.Nf, 1)
		
		# algorithm implementation
		for i in range(Nit):
			for k in range (self.Np):
				grad = (np.dot(A[[k], :], w) - y[k])*A[[k], :].T
				w = w - gamma*grad
			self.err[i] = np.linalg.norm(np.dot(A, w) - y)**2
		
		self.sol = w

				
class Conj(SolveMinProb):
	"""Conjugate Gradient algorithm
	"""
	def run(self):
		"""Algorithm implementation
		"""
		# initialize the element
		A = self.matr
		y = self.vect
		w = np.zeros((self.Nf, 1), dtype = float)
		self.err = np.zeros((self.Np, 1), dtype = float)
		 
		# determine the matrix Q and the vector b
		Q = np.dot(A.T, A)
		b = np.dot(A.T, y)
		d = b
		grad = -b
		
		for k in range(self.Np):
			alfa = -(np.dot(d.T, grad) / np.dot(np.dot(d.T,Q), d))
			w = w + alfa*d
			grad = np.dot(Q, w) - b
			beta = np.dot(np.dot(grad.T, Q), d) / np.dot(np.dot(d.T, Q), d)
			d = -grad + beta*d
			self.err[k] = np.linalg.norm(np.dot(A, w) - y)**2
			
		self.sol = w

		
class RidgeReg(SolveMinProb):
	"""Ridge Regression algorithm implementation
	"""
	def run(self, lamb):
		# initialize the parameters
		A = self.matr
		y = self.vect
		
		# three terms used in the formula
		a_term = np.dot(A.T, A)
		b_term = lamb * np.eye(A.shape[1])
		c_term = np.dot(A.T, y)
		# determine the optimized vector
		w = np.dot(np.linalg.inv(a_term + b_term), c_term)
		
		self.sol = w
	
	def find_lamb(self, lamb_max, y_val, A_val):
		"""Find the optimum Lambda
		
		Args:
			lamb_max: (int) Number of iterations
			y_val: Np-by-1 Numpy vector. Validation vector
			A_val: Np-by-Nf Numpy matrix. Validation matrix
		
		Returns:
			optimum_lamb: (int) value of lambda which minimize the error when the validation 
				dataset it is used
		"""
		A = self.matr
		y = self.vect
		self.err = np.zeros((lamb_max, 1), dtype = float)
		
		for i in range(lamb_max):
			self.run(i)
			self.err[i] = np.linalg.norm(np.dot(A_val, self.sol) - y_val)**2
		optimum_lamb = np.argmin(self.err)
		
		return optimum_lamb
